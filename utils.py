import os
import h5py
import numpy as np
from tqdm import tqdm
import sys
import csv
from pathlib import Path

def count_patches_in_dataset(reactive_dir, malt_dir, min_valid_patches=1):
    """
    ç»Ÿè®¡æ•°æ®é›†ä¸­æ‰€æœ‰æœ‰æ•ˆçš„patchæ•°é‡
    """
    total_patches = 0
    total_files = 0

    def count_from_dir(dir_path, label_name):
        nonlocal total_patches, total_files

        files = [f for f in os.listdir(dir_path) if f.endswith('.h5')]
        dir_total_patches = 0

        print(f"Processing {label_name} files...")
        for f in tqdm(files, desc=f"Counting {label_name}"):
            file_path = os.path.join(dir_path, f)
            try:
                with h5py.File(file_path, 'r') as h5f:
                    if 'features' not in h5f:
                        continue
                    feats = np.array(h5f['features'])
                    if feats.ndim != 2:
                        continue

                    # ç»Ÿè®¡æœ‰æ•ˆpatchæ•°é‡ï¼ˆéå…¨NaNè¡Œï¼‰
                    valid_mask = ~np.isnan(feats).all(axis=1)
                    valid_count = valid_mask.sum()

                    if valid_count >= min_valid_patches:
                        dir_total_patches += valid_count
                        total_files += 1

            except Exception as e:
                print(f"âš ï¸ Skip {file_path}: {e}")

        print(
            f"{label_name} - Files: {len(files)}, Valid files: {total_files if label_name == 'MALT' else total_files - sum(1 for f in os.listdir(reactive_dir) if f.endswith('.h5'))}, Patches: {dir_total_patches}")
        return dir_total_patches

    # ç»Ÿè®¡ä¸¤ä¸ªç›®å½•ä¸‹çš„patchæ•°é‡
    reactive_patches = count_from_dir(reactive_dir, "Reactive")
    malt_patches = count_from_dir(malt_dir, "MALT")

    total_patches = reactive_patches + malt_patches

    print(f"\n=== ç»Ÿè®¡ç»“æœ ===")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æ€»patchæ•°: {total_patches}")
    print(f"Reactive patchæ•°: {reactive_patches}")
    print(f"MALT patchæ•°: {malt_patches}")
    print(f"å¹³å‡æ¯æ–‡ä»¶patchæ•°: {total_patches / total_files:.2f}" if total_files > 0 else "æ— æœ‰æ•ˆæ–‡ä»¶")

    return total_patches


def inspect_h5_feature(filepath):
    print(f"ğŸ” Inspecting features in: {filepath}")
    print("=" * 60)

    try:
        with h5py.File(filepath, 'r') as f:
            # åˆ—å‡ºæ‰€æœ‰é¡¶å±‚é”®
            print("Top-level keys:", list(f.keys()))

            if 'features' not in f:
                print("âŒ Error: 'features' dataset not found!")
                return

            features = np.array(f['features'])
            print(f"\nâœ… Found 'features' dataset:")
            print(f"   Shape: {features.shape}")
            print(f"   Dtype: {features.dtype}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸€ç»´å‘é‡ï¼ˆå¦‚ 768ï¼‰
            if features.ndim != 1:
                print(f"âš ï¸  Warning: features is not 1D (ndim={features.ndim})")

            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¯¹æ•°å€¼å‹æœ‰æ•ˆï¼‰
            if np.issubdtype(features.dtype, np.number):
                print(f"   Min: {np.min(features):.6f}")
                print(f"   Max: {np.max(features):.6f}")
                print(f"   Mean: {np.mean(features):.6f}")
                print(f"   Std: {np.std(features):.6f}")

                # æ£€æŸ¥ NaN / Inf
                nan_count = np.isnan(features).sum()
                inf_count = np.isinf(features).sum()
                print(f"   NaN count: {nan_count}")
                print(f"   Inf count: {inf_count}")

                if nan_count > 0 or inf_count > 0:
                    print("   âš ï¸  WARNING: Abnormal values detected!")

            # æ‰“å°å‰10ä¸ªå€¼ï¼ˆä¾¿äºäººå·¥æŸ¥çœ‹ï¼‰
            print(f"\n   First 10 values:")
            print("   ", features[:10])

            # å¦‚æœå¤ªé•¿ï¼Œä¹Ÿæ‰“å°æœ€å5ä¸ª
            if len(features) > 20:
                print(f"   Last 5 values:")
                print("   ", features[-5:])

    except Exception as e:
        print(f"âŒ Failed to read {filepath}: {e}")
        sys.exit(1)


def inspect_npy_feature(filepath):
    print(f"ğŸ” Inspecting features in: {filepath}")
    print("=" * 60)

    try:
        features = np.load(filepath)
        print(f"\nâœ… Loaded .npy file:")
        print(f"   Shape: {features.shape}")
        print(f"   Dtype: {features.dtype}")

        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¯¹æ•°å€¼å‹æœ‰æ•ˆï¼‰
        if np.issubdtype(features.dtype, np.number):
            print(f"   Min: {np.min(features):.6f}")
            print(f"   Max: {np.max(features):.6f}")
            print(f"   Mean: {np.mean(features):.6f}")
            print(f"   Std: {np.std(features):.6f}")

            # æ£€æŸ¥ NaN / Inf
            nan_count = np.isnan(features).sum()
            inf_count = np.isinf(features).sum()
            print(f"   NaN count: {nan_count}")
            print(f"   Inf count: {inf_count}")

            if nan_count > 0 or inf_count > 0:
                print("   âš ï¸  WARNING: Abnormal values detected!")

        # æ‰“å°å‰10ä¸ªå€¼ï¼ˆä¾¿äºäººå·¥æŸ¥çœ‹ï¼‰
        print(f"\n   First 10 values:")
        print("   ", features.flat[:10] if features.ndim > 1 else features[:10])

        # å¦‚æœå¤ªé•¿ï¼Œä¹Ÿæ‰“å°æœ€å5ä¸ª
        if features.size > 20:
            print(f"   Last 5 values:")
            print("   ", features.flat[-5:] if features.ndim > 1 else features[-5:])

    except Exception as e:
        print(f"âŒ Failed to read {filepath}: {e}")
        sys.exit(1)


def read_coords_legacy(coords_path):
    with h5py.File(coords_path, 'r') as f:
        patch_size = f['coords'].attrs['patch_size']
        patch_level = f['coords'].attrs['patch_level']
        custom_downsample = f['coords'].attrs.get('custom_downsample', 1)
        coords = f['coords'][:]
        print("ğŸ“‹ Legacy Coords Info:")
        print(f"   Patch Size: {patch_size}")
        print(f"   Patch Level: {patch_level}")
        print(f"   Custom Downsample: {custom_downsample}")
        print(f"   Number of Coords: {coords.shape[0]}")
        print(f"   First 5 Coords:\n{coords[:5]}")

def extract_slide_id_from_filename(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–ç—…ç†å·ï¼šå– "-HE" ä¹‹å‰çš„éƒ¨åˆ†ï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
    ä¾‹å¦‚ï¼šS12345-HE.kfb â†’ S12345
    """
    stem = Path(filename).stem  # å»æ‰æ‰©å±•å
    if "-HE" in stem:
        slide_id = stem.split("-HE")[0]
        return slide_id
    else:
        return None  # æˆ–è€…å¯ä»¥ raise ValueError(f"æ–‡ä»¶åä¸å« '-HE': {filename}")


def collect_wsi_slide_ids(root_dir_positive, root_dir_negative, output_csv="slide_labels.csv"):
    """
    éå†ä¸¤ä¸ªæ ¹ç›®å½•ï¼ˆåŠå…¶å„è‡ªçš„ WSI/HE å­ç›®å½•ï¼‰ï¼Œæå–ç—…ç†å·å¹¶æ‰“æ ‡ç­¾ã€‚

    Args:
        root_dir_positive (str): positive æ ·æœ¬çš„æ ¹ç›®å½•
        root_dir_negative (str): negative æ ·æœ¬çš„æ ¹ç›®å½•
        output_csv (str): è¾“å‡º CSV æ–‡ä»¶å
    """
    extensions = {'.sdpc', '.kfb'}
    records = []

    def scan_directory(base_path, label):
        """æ‰«æ base_path åŠå…¶ WSI/HE å­ç›®å½•"""
        base = Path(base_path)
        if not base.exists():
            print(f"è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            return

        # æ‰«æ base_path æœ¬èº«
        for file in base.iterdir():
            if file.is_file() and file.suffix.lower() in extensions:
                slide_id = extract_slide_id_from_filename(file.name)
                if slide_id:
                    records.append((slide_id, label))
                else:
                    print(f"è·³è¿‡æ—  '-HE' çš„æ–‡ä»¶: {file}")

        # æ‰«æ WSI/HE å­ç›®å½•
        he_dir = base / "WSI" / "HE"
        if he_dir.exists() and he_dir.is_dir():
            for file in he_dir.iterdir():
                if file.is_file() and file.suffix.lower() in extensions:
                    slide_id = extract_slide_id_from_filename(file.name)
                    if slide_id:
                        records.append((slide_id, label))
                    else:
                        print(f"è·³è¿‡æ—  '-HE' çš„æ–‡ä»¶: {file}")
        else:
            print(f"æç¤º: æœªæ‰¾åˆ° {he_dir} ç›®å½•ï¼Œè·³è¿‡å­ç›®å½•æ‰«æ")

    # å¤„ç† positive è·¯å¾„
    scan_directory(root_dir_positive, 'positive')
    # å¤„ç† negative è·¯å¾„
    scan_directory(root_dir_negative, 'negative')

    # å»é‡ï¼šä¿ç•™é¦–æ¬¡å‡ºç°çš„ slide_idï¼ˆé¿å…é‡å¤æ ‡æ³¨ï¼‰
    seen = set()
    unique_records = []
    for slide_id, label in records:
        if slide_id not in seen:
            unique_records.append((slide_id, label))
            seen.add(slide_id)
        else:
            print(f"âš ï¸ é‡å¤ç—…ç†å·: {slide_id}ï¼Œå·²å¿½ç•¥åç»­å‡ºç°ã€‚")

    # å†™å…¥ CSV
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['slide_id', 'label'])
        writer.writerows(unique_records)

    print(f"âœ… å·²ç”Ÿæˆ {output_csv}ï¼Œå…± {len(unique_records)} æ¡è®°å½•ã€‚")


if __name__ == "__main__":
    # ä½¿ç”¨ä¸ABMIL.pyç›¸åŒçš„è·¯å¾„
    reactive_dir = "/mnt/gml/GML/Project/Trident/HE/Reactive/20x_224px_0px_overlap/features_virchow"
    malt_dir = "/mnt/gml/GML/Project/Trident/HE/MALT/20x_224px_0px_overlap/features_virchow"

    print("å¼€å§‹ç»Ÿè®¡æ•°æ®é›†ä¸­çš„patchæ•°é‡...")
    count_patches_in_dataset(reactive_dir, malt_dir)